{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LSTM_PRESENTATION-RESIDUAL AND TREND.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J94SUgR-4iaY"
      },
      "source": [
        "LSTM FOR FORECASTING TB CASES AT KIDH"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jgGUTO5jstlA"
      },
      "source": [
        "INSTALL NECESSARY LIBRARY"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "PnfF2Z-8kvJJ"
      },
      "source": [
        "#pip install --ignore-installed qiskit-terra qiskit-aer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EfzBpkBqgtpA"
      },
      "source": [
        "#pip install numpy==1.19.5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "350wnaAbdkQR"
      },
      "source": [
        "pip install -U scikit-learn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4dUJu5VYsbqL"
      },
      "source": [
        "DATASET UPLOAD TO COLAB"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WHj9xzb6C0_F"
      },
      "source": [
        "IMPORT NECCESSARY LIBRARY"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hcTHaKmQvWPX"
      },
      "source": [
        "#upload the dataset into google colab\n",
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sQ_a39ql4j9p"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import Bidirectional\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from keras.layers.core import Dense, Activation, Dropout\n",
        "import time #helper libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import mean_absolute_percentage_error\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.metrics import r2_score\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "plt.rcParams.update({'figure.figsize':(9,6), 'figure.dpi':100})\n",
        "plt.style.use('fivethirtyeight')\n",
        "plt.rcParams['axes.labelsize'] = 12\n",
        "plt.rcParams['xtick.labelsize'] = 10\n",
        "plt.rcParams['ytick.labelsize'] = 10\n",
        "plt.rcParams['text.color'] = 'G'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ctzhwI5n-a0c"
      },
      "source": [
        "READ THE DATASET"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AdhxupzoDBHv"
      },
      "source": [
        "df = pd.read_csv('residual_new.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H5B665Sfw7Uk"
      },
      "source": [
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QWglhRDlFsZM"
      },
      "source": [
        "DATA PREPROSSING"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e9qsnziBAqXD"
      },
      "source": [
        "df['Date']= pd.to_datetime(df['date'],format=\"%d/%m/%y\")\n",
        "df.drop(columns=['date'], inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uDypseHv-Wcr"
      },
      "source": [
        "series=df.set_index('Date')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bgNqs9su0h7Y"
      },
      "source": [
        "series.plot(figsize = (19, 8),title=\"Monthly New and Relapse (bacteriologically confirmed or clinical diagnosis) TB incidence at Kibongoto Infectious Disease Hospital  from 2015 to 2020.\",ylabel=\"New and Relapse TB cases\",xlabel=\"Year\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VFjUG9ZL-qGH"
      },
      "source": [
        "# fix random seed for reproducibility\n",
        "np.random.seed(7)\n",
        "# load the dataset\n",
        "dataset = series.values\n",
        "dataset = dataset.astype('float32')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LC5fYfAV1Zax"
      },
      "source": [
        "# convert an array of values into a dataset matrix\n",
        "def create_dataset(dataset, look_back=1):\n",
        "\tdataX, dataY = [], []\n",
        "\tfor i in range(len(dataset)-look_back-1):\n",
        "\t\ta = dataset[i:(i+look_back), 0]\n",
        "\t\tdataX.append(a)\n",
        "\t\tdataY.append(dataset[i + look_back, 0])\n",
        "\treturn np.array(dataX), np.array(dataY)\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nfSkXkia1iPg"
      },
      "source": [
        "# normalize the dataset\n",
        "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
        "dataset = scaler.fit_transform(dataset)\n",
        "# split into train and test sets\n",
        "train_size = int(len(dataset) * 0.7)\n",
        "test_size = len(dataset) - train_size\n",
        "train, test = dataset[0:train_size,:], dataset[train_size:len(dataset),:]\n",
        "# reshape into X=t and Y=t+1\n",
        "look_back = 2\n",
        "trainX, trainY = create_dataset(train, look_back)\n",
        "testX, testY = create_dataset(test, look_back)\n",
        "# reshape input to be [samples, time steps, features]\n",
        "trainX = np.reshape(trainX, (trainX.shape[0], trainX.shape[1], 1))\n",
        "testX = np.reshape(testX, (testX.shape[0], testX.shape[1], 1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "loF9itQBGTKL"
      },
      "source": [
        "testX.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "crKMqE1dODvY"
      },
      "source": [
        "CREATE MODEL AND FITS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dU7IevCZOA0p"
      },
      "source": [
        "batch_size = 1\n",
        "model = Sequential()\n",
        "model.add(LSTM(4, batch_input_shape=(batch_size, look_back, 1), stateful=True,return_sequences=True))\n",
        "#model.add(Dropout(0.2))\n",
        "model.add(LSTM(4, batch_input_shape=(batch_size, look_back, 1), stateful=True))\n",
        "#model.add(Dropout(0.2))\n",
        "model.add(Dense(1))\n",
        "model.compile(loss='mean_squared_error', optimizer='adam')\n",
        "for i in range(500):\n",
        "\tmodel.fit(trainX, trainY, epochs=1, batch_size=batch_size, verbose=2, shuffle=False)\n",
        "\tmodel.reset_states()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YvcUD5bH2WKn"
      },
      "source": [
        "# make forecast\n",
        "trainPredict = model.predict(trainX, batch_size=batch_size)\n",
        "model.reset_states()\n",
        "testPredict = model.predict(testX, batch_size=batch_size)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vvtapCqE2YWr"
      },
      "source": [
        "# invert predictions\n",
        "trainPredict = scaler.inverse_transform(trainPredict)\n",
        "trainY = scaler.inverse_transform([trainY])\n",
        "testPredict = scaler.inverse_transform(testPredict)\n",
        "testY = scaler.inverse_transform([testY])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KN23FtFDuntv"
      },
      "source": [
        "EVALUATE A MODEL"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BvKShbJ1EoWv"
      },
      "source": [
        "TRAINING PERFOMANCE METRIC"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Z_aJuq5DmZc"
      },
      "source": [
        "# Accuracy metrics\n",
        "def forecast_accuracy(forecast, actual):\n",
        "    mape = mean_absolute_percentage_error(actual, forecast)  # MAPE\n",
        "    mse=mean_squared_error(actual, forecast, squared=False)\n",
        "    mae = mean_absolute_error(actual, forecast)    # MAE   # MPE\n",
        "    rmse = np.sqrt(mean_squared_error(actual, forecast))\n",
        "    r2=r2_score(actual, forecast)  # RMSE\n",
        "    return({'mape':mape,  'mae': mae, \"mse\":mse,\n",
        "             'rmse':rmse,'r2_score':r2})\n",
        "\n",
        "forecast_accuracy(trainPredict[:,0],trainY[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PsDR3Sw1EUZn"
      },
      "source": [
        "TESTING PERFOMANCE METRIC"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tHZ5DgQvDz2Z"
      },
      "source": [
        "# Accuracy metrics\n",
        "def forecast_accuracy(forecast, actual):\n",
        "    mape = mean_absolute_percentage_error(actual, forecast)  # MAPE\n",
        "    mse=mean_squared_error(actual, forecast, squared=False)\n",
        "    mae = mean_absolute_error(actual, forecast)    # MAE   # MPE\n",
        "    rmse = np.sqrt(mean_squared_error(actual, forecast))\n",
        "    r2=r2_score(actual, forecast)  # RMSE\n",
        "    return({'mape':mape,  'mae': mae, \"mse\":mse,\n",
        "             'rmse':rmse,'r2_score':r2})\n",
        "\n",
        "forecast_accuracy(testPredict[:,0],testY[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LNXPIV9iE2O5"
      },
      "source": [
        "TESTING"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9H_TmZNBtGyg"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "from keras.models import Sequential\n",
        "\n",
        "from keras.layers import Dense\n",
        "\n",
        "from keras.callbacks import EarlyStopping\n",
        "\n",
        "from keras.optimizers import Adam\n",
        "\n",
        "from keras.layers import LSTM"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XhTJM99ar5NE"
      },
      "source": [
        "series"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0x2vkSd6sBuv"
      },
      "source": [
        "split_date = pd.Timestamp('2019-01-01')\n",
        "\n",
        "df =  series['tbcases']\n",
        "\n",
        "train = df.loc[:split_date]\n",
        "\n",
        "test = df.loc[split_date:]\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "ax = train.plot()\n",
        "\n",
        "test.plot(ax=ax)\n",
        "\n",
        "plt.legend(['train', 'test']);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NRgI3qB7sKLD"
      },
      "source": [
        "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
        "train=train.values\n",
        "train=train.reshape(-1,1)\n",
        "train_sc = scaler.fit_transform(train)\n",
        "test=test.values\n",
        "test=test.reshape(-1,1)\n",
        "test_sc = scaler.transform(test)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a869I8ZIsxpW"
      },
      "source": [
        "X_train = train_sc[:-1]\n",
        "\n",
        "y_train = train_sc[1:]\n",
        "\n",
        "\n",
        "\n",
        "X_test = test_sc[:-1]\n",
        "\n",
        "y_test = test_sc[1:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v3dB-8cps64C"
      },
      "source": [
        "nn_model = Sequential()\n",
        "\n",
        "nn_model.add(Dense(512, input_dim=1, activation='tanh'))\n",
        "nn_model.add(Dense(512, input_dim=1, activation='tanh'))\n",
        "nn_model.add(Dense(1))\n",
        "\n",
        "nn_model.compile(loss='mean_squared_error', optimizer='adam')\n",
        "\n",
        "early_stop = EarlyStopping(monitor='loss', patience=2, verbose=1)\n",
        "\n",
        "history = nn_model.fit(X_train, y_train, epochs=100, batch_size=1, verbose=1, callbacks=[early_stop], shuffle=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UcslqOcJthnc"
      },
      "source": [
        "y_pred_test_nn = nn_model.predict(X_test)\n",
        "\n",
        "y_train_pred_nn = nn_model.predict(X_train)\n",
        "\n",
        "print(\"The R2 score on the Train set is:\\t{:0.3f}\".format(r2_score(y_train, y_train_pred_nn)))\n",
        "\n",
        "print(\"The R2 score on the Test set is:\\t{:0.3f}\".format(r2_score(y_test, y_pred_test_nn)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pSUHgY2yuOJx"
      },
      "source": [
        "X_train_lmse = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))\n",
        "X_test_lmse = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))\n",
        "\n",
        "lstm_model = Sequential()\n",
        "lstm_model.add(LSTM(100, input_shape=(1, X_train_lmse.shape[1]), activation='tanh', kernel_initializer='lecun_uniform', return_sequences=False))\n",
        "lstm_model.add(Dense(1))\n",
        "lstm_model.compile(loss='mean_squared_error', optimizer='adam')\n",
        "early_stop = EarlyStopping(monitor='loss', patience=2, verbose=1)\n",
        "history_lstm_model = lstm_model.fit(X_train_lmse, y_train, epochs=100, batch_size=1, verbose=1, shuffle=False, callbacks=[early_stop])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iYhcoWaVuVPo"
      },
      "source": [
        "y_pred_test_lstm = lstm_model.predict(X_test_lmse)\n",
        "y_train_pred_lstm = lstm_model.predict(X_train_lmse)\n",
        "print(\"The R2 score on the Train set is:\\t{:0.3f}\".format(r2_score(y_train, y_train_pred_lstm)))\n",
        "print(\"The R2 score on the Test set is:\\t{:0.3f}\".format(r2_score(y_test, y_pred_test_lstm)))\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}